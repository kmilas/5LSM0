{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1QFpGuExAth"
      },
      "source": [
        "# Lecture 8: Generative Models\n",
        "\n",
        "Welcome to this Week 8 hands-on tutorial! In this notebook, we’ll explore how to train Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) using **PyTorch**. These models are powerful tools for generating new data samples and learning complex data distributions.\n",
        "\n",
        "For this task, we will build and train both a **VAE** and a **GAN**:\n",
        "\n",
        "- **Variational Autoencoder (VAE)**: A probabilistic model that represents input data in a latent space by learning both reconstruction and a prior distribution, useful for generative modeling.  \n",
        "  - **Encoder**: Maps input data to the parameters (mean and variance) of a latent probability distribution.  \n",
        "  - **Decoder**: Reconstructs data from samples drawn from the latent distribution.  \n",
        "  - **Latent Space**: A probabilistic representation of the data distribution that allows for structured sampling and interpolation.  \n",
        "\n",
        "\n",
        "- **Generative Adversarial Network (GAN)**: Consists of two neural networks, a generator and a discriminator, that compete against each other. The generator creates fake data samples, while the discriminator tries to distinguish between real and fake samples.\n",
        "  - **Generator**: Generates new data samples from random noise.\n",
        "  - **Discriminator**: Classifies data samples as real or fake.\n",
        "  - **Adversarial Training**: The generator and discriminator are trained simultaneously, improving each other’s performance.\n",
        "\n",
        "By the end of this notebook, you’ll understand how to:\n",
        "1. Preprocess data for training VAEs and GANs.\n",
        "2. Build and train a VAE using PyTorch.\n",
        "3. Build and train a GAN using PyTorch.\n",
        "4. Generate new data samples.\n",
        "\n",
        "Let’s dive in and start building!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfmPUH63xAtk"
      },
      "source": [
        "## Variational Autoncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3Ut62LcxAtl"
      },
      "source": [
        "### Step 1: Import libraries and load the MNIST dataset\n",
        "\n",
        "In this step, we’ll prepare our tools and data to kickstart the project. First, we’ll import the necessary libraries that make it possible to handle data, build and train neural networks, and visualize results effectively.\n",
        "\n",
        "Next, we’ll load the **MNIST dataset** using `torchvision`. As a quick refresher, the MNIST dataset contains 70,000 grayscale images (28x28 pixels) of handwritten digits, distributed across 10 classes (digits 0 through 9). Each class has a varying number of images, providing a diverse dataset for training.\n",
        "\n",
        "To reduce computational requirements, we’ll **resize** the images and **flatten** them. Resizing adjusts the dimensions of the images, and flattening converts the 2D images into 1D vectors, making it easier for the model to process and speeding up the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6aNNQTIgxAtl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm, trange\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1qETq70dxAtn",
        "outputId": "4a6683f3-0c65-4f39-ba5a-b6922bf900b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 19.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 644kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 2.80MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.26MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((14, 14), interpolation=0),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "\n",
        "def plot_dataset(X, **kwargs):\n",
        "    '''\n",
        "    Plot a 2D dataset.\n",
        "    Args:\n",
        "        X: torch.Tensor: The images to plot.\n",
        "        **kwargs: dict: Additional keyword arguments to pass to `ax.set`.\n",
        "    '''\n",
        "    # reshape the data to 14x14 images\n",
        "    X = X.view(-1, 14, 14)\n",
        "    # get 16 samples from the batch\n",
        "    X = X[:16].unsqueeze(1)\n",
        "    # make a grid of the images\n",
        "    grid = make_grid(X, nrow=4, normalize=True)\n",
        "    # plot the image grid\n",
        "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
        "    plt.axis('off')\n",
        "    if kwargs.get('title', None):\n",
        "        plt.title(kwargs.get('title', ''))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Plot images from the dataset\n",
        "X, _ = next(iter(train_dataloader))\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plot_dataset(X, title='MNIST Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "markdown"
        },
        "id": "Rfc5RyujxAtn"
      },
      "source": [
        "### Step 2: Building the VAE Components\n",
        "\n",
        "In this step, we will build and implement the core components of a Variational Autoencoder (VAE): the encoder, the decoder, and the reparametrization trick.\n",
        "\n",
        "#### Encoder\n",
        "The encoder compresses the input data into a latent representation. It consists of several layers that reduce the dimensionality of the input data.\n",
        "\n",
        "#### Decoder\n",
        "The decoder reconstructs the data from the latent representation. It consists of several layers that expand the latent representation back to the original data dimensions.\n",
        "\n",
        "#### Reparametrization Trick\n",
        "The reparametrization trick allows us to sample from the latent space distribution using the mean and log variance estimated from the features extracted by the encoder. This trick ensures that the gradient can flow through the sampling process during backpropagation.\n",
        "\n",
        "The reparametrization trick is defined as:\n",
        "$$ z = \\mu + \\sigma \\cdot \\epsilon $$\n",
        "where:\n",
        "- $ \\mu $ is the mean estimated by the encoder.\n",
        "- $ \\sigma $ is the standard deviation, computed as $ \\sigma = \\exp(\\frac{1}{2} \\cdot \\text{logvar}) $.\n",
        "- $ \\epsilon $ is a random noise vector sampled from a standard normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aqm_nIgBxAto"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_features, out_features, normalize=False):\n",
        "        '''\n",
        "        Create a block of an MLP.\n",
        "        Args:\n",
        "            in_features: int: The number of input features.\n",
        "            out_features: int: The number of output features.\n",
        "            normalize: bool: Whether to apply batch normalization.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(in_features, out_features)) # replace with your implementation of a layer that takes in_features and outputs out_features\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(out_features, 0.8))\n",
        "        layers.append(nn.ReLU(inplace=True))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The output features.\n",
        "        '''\n",
        "        return self.block(X)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, image_size=14):\n",
        "        '''\n",
        "        Create an MLP to generate samples from a latent space.\n",
        "        Args:\n",
        "            image_size: int: The size of the images.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Hint: use the Block class to create the layers of the encoder\n",
        "        self.model = nn.Sequential(\n",
        "            Block(image_size**2, 32), # replace with your implementation of a layer that takes the flattened image and outputs 32 features (do not apply normalization)\n",
        "            Block(32, 64), # replace with your implementation of a layer that takes 32 features and outputs 64 features\n",
        "            Block(64, 128), # replace with your implementation of a layer that takes 64 features and outputs 128 features\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The encoded features.\n",
        "        '''\n",
        "        X = self.model(X)\n",
        "        return X\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, image_size=14, latent_dim=16):\n",
        "        '''\n",
        "        Create an MLP to generate samples from a latent space.\n",
        "        Args:\n",
        "            image_size: int: The size of the images.\n",
        "            latent_dim: int: The dimensionality of the latent space.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Hint: use the Block class to create the layers of the decoder\n",
        "        self.model = nn.Sequential(\n",
        "            Block(latent_dim, 128), # replace with your implementation of a layer that takes latent_dim features and outputs 128 features\n",
        "            Block(128, 64), # replace with your implementation of a layer that takes 128 features and outputs 64 features\n",
        "            Block(64, 32), # replace with your implementation of a layer that takes 64 features and outputs 32 features\n",
        "            Block(32, image_size**2), # without using Block, implement a layer that takes 32 features and outputs image_size*image_size features\n",
        "            nn.Tanh(), # Final activation layer to output the image in range [-1,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The generated samples.\n",
        "        '''\n",
        "        X = self.model(X)\n",
        "        return X\n",
        "\n",
        "class VariationalAutoEncoder(nn.Module):\n",
        "    def __init__(self, image_size=14, latent_dim=8):\n",
        "        '''\n",
        "        Create a VAE.\n",
        "        Args:\n",
        "            image_size: int: The size of the images.\n",
        "            latent_dim: int: The dimensionality of the latent space.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(image_size)\n",
        "        self.mu = nn.Linear(128, latent_dim) # Without using Block, implement layer that takes 128 features and outputs latent_dim features\n",
        "        self.logvar = nn.Linear(128, latent_dim)  # Without using Block, implement layer that takes 128 features and outputs latent_dim features\n",
        "        self.decoder = Decoder(image_size, latent_dim)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            X: torch.Tensor: The generated samples.\n",
        "            mu: torch.Tensor: The mean of the latent space.\n",
        "            logvar: torch.Tensor: The log variance of the latent space.\n",
        "        '''\n",
        "        X = self.encoder(X)\n",
        "\n",
        "        mu = self.mu(X)\n",
        "        logvar = self.logvar(X)\n",
        "\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "\n",
        "        X = mu + std*eps # replace with your implementation of the reparameterization trick\n",
        "        X = self.decoder(X)\n",
        "\n",
        "        return X, mu, logvar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulW1ADyqxAtp"
      },
      "source": [
        "### Step 3: Defining the Loss Functions and Training the VAE\n",
        "\n",
        "In this step, we will define the loss functions required to train the VAE: the KL divergence loss and the global VAE loss (Evidence Lower Bound, ELBO). We will then train the model using these loss functions.\n",
        "\n",
        "### KL Divergence\n",
        "The KL divergence loss measures the difference between the learned latent distribution and a standard normal distribution. It ensures that the latent space follows a normal distribution, which is crucial for generating new data samples.\n",
        "\n",
        "The KL divergence for a single data point can be determined using\n",
        "$$ \\mathcal{L}_{\\text{KL}} = \\frac{1}{2} \\sum(1 + \\text{logvar} - \\mu^2 - \\exp(\\text{logvar})) $$\n",
        "\n",
        "### Reconstruction Loss\n",
        "The reconstruction loss measures how well the decoder can reconstruct the input data from the latent representation. It is typically defined as the mean squared error (MSE) or binary cross-entropy (BCE) between the input and the reconstructed output.\n",
        "\n",
        "### Global VAE Loss (ELBO)\n",
        "The global VAE loss, also known as the Evidence Lower Bound (ELBO), combines the reconstruction loss and the KL divergence loss. Minimizing the ELBO ensures that the model learns to reconstruct the data accurately while maintaining a well-structured latent space.\n",
        "\n",
        "The ELBO is defined as\n",
        "$$ \\text{ELBO} = \\text{Reconstruction Loss} + \\text{KL Divergence Loss} $$\n",
        "\n",
        "### Training the Model\n",
        "To train the VAE, we will:\n",
        "1. Perform a forward pass through the model to obtain the reconstructed image, the mean and the log variance.\n",
        "2. Compute the reconstruction loss and the KL divergence loss.\n",
        "3. Combine the losses to compute the ELBO.\n",
        "4. Perform backpropagation and update the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oxXyFYvkxAtp",
        "outputId": "f9441eb5-4a25-45a7-ea39-85ae9be2c956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [00:19<02:53, 19.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.9257346987724304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:37<02:30, 18.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 0.926478922367096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:57<02:13, 19.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 0.919318437576294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:15<01:53, 18.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 0.9237011075019836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:36<01:38, 19.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 0.9261587858200073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:55<01:17, 19.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 0.9247496724128723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:17<01:00, 20.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 0.9225887656211853\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:36<00:39, 19.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 0.92283034324646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:55<00:19, 19.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 0.9192891716957092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [03:14<00:00, 19.43s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 0.9224399328231812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def kl_loss(mu, logvar):\n",
        "    '''\n",
        "    Compute the KL divergence loss.\n",
        "    Args:\n",
        "        mu: torch.Tensor: The mean of the latent space.\n",
        "        logvar: torch.Tensor: The log variance of the latent space.\n",
        "    Returns:\n",
        "        torch.Tensor: The KL divergence loss.\n",
        "    '''\n",
        "    kl_loss = - (1+logvar - mu*mu - torch.exp(logvar) ) # replace with your implementation of the KL divergence loss\n",
        "    return kl_loss.mean()\n",
        "\n",
        "def vae_loss(X, X_hat, mu, logvar, kl_weight=0.01):\n",
        "    '''\n",
        "    Compute the VAE loss.\n",
        "    Args:\n",
        "        X: torch.Tensor: The input features.\n",
        "        X_hat: torch.Tensor: The reconstructed features.\n",
        "        mu: torch.Tensor: The mean of the latent space.\n",
        "        logvar: torch.Tensor: The log variance of the latent space.\n",
        "    Returns:\n",
        "        torch.Tensor: The VAE loss.\n",
        "    '''\n",
        "    return nn.MSELoss()(X, X_hat) + kl_weight*kl_loss(mu, logvar)\n",
        "\n",
        "def train_vae(vae, dataloader, epochs=10, lr=0.001):\n",
        "    '''\n",
        "    Train a VAE.\n",
        "    Args:\n",
        "        vae: VariationalAutoEncoder: The VAE to train.\n",
        "        dataloader: DataLoader: The DataLoader for the dataset.\n",
        "        epochs: int: The number of epochs to train for.\n",
        "        lr: float: The learning rate to use.\n",
        "    '''\n",
        "    optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
        "    for epoch in trange(epochs):\n",
        "        for X, _ in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            X_hat, mu, logvar = vae(X)\n",
        "            loss = vae_loss(X, X_hat, mu, logvar, kl_weight=0.01)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Create the VAE and train it.\n",
        "vae = VariationalAutoEncoder(latent_dim=16)\n",
        "train_vae(vae, train_dataloader, epochs=10, lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnnwS4TFxAtq"
      },
      "source": [
        "### Step 4: Sampling from the VAE\n",
        "\n",
        "In this step, we will sample new data from the trained VAE model. Sampling involves generating new latent vectors from the learned latent space and passing them through the decoder to produce new data samples.\n",
        "\n",
        "#### Sampling Process\n",
        "1. **Generate Latent Vectors**: Sample random vectors from a standard normal distribution. These vectors represent points in the latent space.\n",
        "2. **Decode Latent Vectors**: Pass the sampled latent vectors through the decoder to generate new data samples.\n",
        "\n",
        "By following this process, we can generate new data samples that are similar to the training data but not identical, demonstrating the generative capabilities of the VAE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dZJxYGaexAtr",
        "outputId": "0c91174d-da72-44b9-f912-c8afca6aa1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def sample_vae(vae, n_samples=16):\n",
        "    '''\n",
        "    Sample from a VAE.\n",
        "    Args:\n",
        "        vae: VariationalAutoEncoder: The VAE to sample from.\n",
        "        n_samples: int: The number of samples to generate.\n",
        "    Returns:\n",
        "        torch.Tensor: The generated samples.\n",
        "    '''\n",
        "    vae.eval()\n",
        "    z = torch.randn(n_samples,16)# sample from a standard normal distribution\n",
        "    samples = vae.decoder(z)\n",
        "    return samples\n",
        "\n",
        "samples = sample_vae(vae)\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plot_dataset(samples, title='Samples from the VAE')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-bCIT5YxAtr"
      },
      "source": [
        "## Generative Adversarial Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sTcDPrrxAtr"
      },
      "source": [
        "### Step 5: Using a Single Digit for GAN Training\n",
        "\n",
        "To make the training process of the GAN faster, we will use only one digit from the MNIST dataset. This will reduce the complexity of the data and allow the GAN to focus on generating variations of a single digit.\n",
        "\n",
        "#### Selecting a Single Digit\n",
        "We will filter the MNIST dataset to include only images of a specific digit (e.g., digit '0'). This will create a smaller and more focused dataset for training the GAN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H290tfdnxAtr"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((14, 14), interpolation=0),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "    transforms.Lambda(lambda x: x.view(-1))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# lets use ibky the digit 5\n",
        "train_dataset.data = train_dataset.data[train_dataset.targets == 5]\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "# Plot images from the dataset\n",
        "X, _ = next(iter(train_dataloader))\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "plot_dataset(X, title='MNIST Digit 5 Dataset')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1-WP1eaxAts"
      },
      "source": [
        "### Step 6: Building the Generator and Discriminator of the GAN\n",
        "\n",
        "In this step, we will build the generator and discriminator models for the GAN. The generator will create fake data samples from random noise, while the discriminator will classify data samples as real or fake.\n",
        "\n",
        "#### Generator\n",
        "The generator takes a random noise vector as input and transforms it into a data sample that resembles the real data. It typically consists of several layers of transposed convolutions or fully connected layers. The final activation function used in the generator is usually a **Tanh** function when the goal is image generation. This activation function scales the output to a range between -1 and 1, which helps in stabilizing the training process and ensuring that the generated data has similar properties to the real data.\n",
        "\n",
        "#### Discriminator\n",
        "The discriminator takes a data sample as input and outputs a probability indicating whether the sample is real or fake. It typically consists of several layers of convolutions or fully connected layers. The final activation function used in the discriminator is usually a **Sigmoid** function. This activation function outputs a probability between 0 and 1, which is suitable for binary classification tasks where the goal is to distinguish between real and fake samples.\n",
        "\n",
        "By defining these models with appropriate final activation functions, we can ensure that the generator produces realistic data samples and the discriminator effectively distinguishes between real and fake samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY9uZNZZxAts"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_features, out_features, normalize=True):\n",
        "        '''\n",
        "        Create a block of an MLP.\n",
        "        Args:\n",
        "            in_features: int: The number of input features.\n",
        "            out_features: int: The number of output features.\n",
        "            normalize: bool: Whether to apply batch normalization.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        layers.append(None) # replace with your implementation of a layer that takes in_features and outputs out_features\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm1d(out_features, 0.8))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The output features.\n",
        "        '''\n",
        "        return self.block(X)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, image_size=14, latent_dim=8):\n",
        "        '''\n",
        "        Create an MLP to generate samples from a latent space.\n",
        "        Args:\n",
        "            image_size: int: The size of the images.\n",
        "            latent_dim: int: The dimensionality of the latent space.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Hint: use the Block class to create some of the layers of the generator\n",
        "        self.model = nn.Sequential(\n",
        "            None, # replace with your implementation of a layer that takes latent_dim features and outputs latent_dim*2 features (do not apply normalization)\n",
        "            None, # replace with your implementation of a layer that takes latent_dim*2 features and outputs latent_dim*4 features\n",
        "            None, # replace with your implementation of a layer that takes latent_dim*4 features and outputs latent_dim*8 features\n",
        "            None, # replace with your implementation of a layer that takes latent_dim*8 features and outputs latent_dim*16 features\n",
        "            None, # Without using Block, implement a layer that takes latent_dim*16 features and outputs image_size*image_size features\n",
        "            None, # Final activation layer to output the image in range [-1,1]\n",
        "        )\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The generated samples.\n",
        "        '''\n",
        "        X = self.model(X)\n",
        "        return X\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, image_size=14):\n",
        "        '''\n",
        "        Create an MLP to discriminate between real and generated samples.\n",
        "        Args:\n",
        "            image_size: int: The size of the images.\n",
        "        '''\n",
        "        super().__init__()\n",
        "\n",
        "        # Hint: use the Block class to create some of the layers of the discriminator\n",
        "        self.model = nn.Sequential(\n",
        "            None, # replace with your implementation of a Block that takes image_size*image_size features and outputs 256 features (do not apply normalization)\n",
        "            None, # replace with your implementation of a Block that takes 256 features and outputs 128 features (do not apply normalization)\n",
        "            None, # without using Block, implement a layer that takes 128 features and outputs 1 feature\n",
        "            None, # Final activation layer to output the probability of the input being real\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        '''\n",
        "        Forward pass of the model.\n",
        "        Args:\n",
        "            X: torch.Tensor: The input features.\n",
        "        Returns:\n",
        "            torch.Tensor: The discriminator's prediction.\n",
        "        '''\n",
        "        X = self.model(X)\n",
        "        return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Zhoo5CxAts"
      },
      "source": [
        "### Step 7: Adversarial Loss, Discriminator Loss, and GAN Training\n",
        "\n",
        "In this step, we will describe the loss functions used to train the GAN and the overall training process.\n",
        "\n",
        "#### Adversarial Loss\n",
        "The adversarial loss is used to train the generator. The goal of the generator is to produce data samples that are indistinguishable from real data, thereby \"fooling\" the discriminator. The adversarial loss for the generator is defined as the negative log probability of the discriminator classifying the generated samples as real:\n",
        "\n",
        "$$ \\mathcal{L}_{\\text{G}} = -\\log(D(G(z))) $$\n",
        "\n",
        "where:\n",
        "- $ G(z) $ is the generated data sample from the generator $ G $ given a random noise vector $ z $.\n",
        "- $ D $ is the discriminator.\n",
        "\n",
        "#### Discriminator Loss\n",
        "The discriminator's goal is to correctly classify real data samples as real and generated data samples as fake. The discriminator loss is a combination of the loss for real samples and the loss for fake samples:\n",
        "\n",
        "$$ \\mathcal{L}_{\\text{D}} = -\\frac{1}{2}\\left[\\log(D(x)) + \\log(1 - D(G(z)))\\right] $$\n",
        "\n",
        "where:\n",
        "- $ x $ is a real data sample.\n",
        "- $ G(z) $ is the generated data sample from the generator $ G $ given a random noise vector $ z $.\n",
        "- $ D(x) $ is the discriminator's probability that $ x $ is real.\n",
        "- $ D(G(z)) $ is the discriminator's probability that the generated sample $ G(z) $ is real.\n",
        "\n",
        "#### GAN Training\n",
        "The training process for the GAN involves alternating between updating the discriminator and the generator:\n",
        "\n",
        "1. **Update the Generator**:\n",
        "   - Sample a batch of random noise vectors $ z $ and generate fake data samples $ G(z) $.\n",
        "   - Compute the adversarial loss $ \\mathcal{L}_{\\text{G}} $ for the generator.\n",
        "   - Perform backpropagation and update the generator's parameters to minimize $ \\mathcal{L}_{\\text{G}} $.\n",
        "\n",
        "2. **Update the Discriminator**:\n",
        "   - Sample a batch of real data samples $ x $ from the dataset.\n",
        "   - Sample a batch of random noise vectors $ z $ and generate fake data samples $ G(z) $.\n",
        "   - Compute the discriminator loss $ \\mathcal{L}_{\\text{D}} $ using both real and fake samples.\n",
        "   - Perform backpropagation and update the discriminator's parameters to minimize $ \\mathcal{L}_{\\text{D}} $."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0oYpvG4xAtt"
      },
      "outputs": [],
      "source": [
        "def adversarial_loss(y_hat):\n",
        "    '''\n",
        "    Compute the adversarial loss.\n",
        "    Args:\n",
        "        y_hat: torch.Tensor: The discriminator's prediction.\n",
        "    Returns:\n",
        "        torch.Tensor: The adversarial loss.\n",
        "    '''\n",
        "    return None # replace with your implementation of the adversarial loss\n",
        "\n",
        "def discriminator_loss(y_real, y_fake):\n",
        "    '''\n",
        "    Compute the discriminator loss.\n",
        "    Args:\n",
        "        y_real: torch.Tensor: The discriminator's prediction for real samples.\n",
        "        y_fake: torch.Tensor: The discriminator's prediction for fake samples.\n",
        "    Returns:\n",
        "        torch.Tensor: The discriminator loss.\n",
        "    '''\n",
        "    return None # replace with your implementation of the discriminator loss\n",
        "\n",
        "def training_loop(generator, discriminator, dataloader, epochs=10, lr=1e-3):\n",
        "    '''\n",
        "    Train a GAN.\n",
        "    Args:\n",
        "        generator: Generator: The generator model.\n",
        "        discriminator: Discriminator: The discriminator model.\n",
        "        dataloader: DataLoader: The DataLoader for the dataset.\n",
        "        epochs: int: The number of epochs to train for.\n",
        "        lr: float: The learning rate to use.\n",
        "    '''\n",
        "    generator_optimizer = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "    discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "    for epoch in trange(epochs, desc='Epochs'):\n",
        "        for X, _ in dataloader:\n",
        "\n",
        "            # Train the generator\n",
        "            generator_optimizer.zero_grad()\n",
        "            z = torch.randn(X.size(0), generator.latent_dim)\n",
        "            X_fake = generator(z)\n",
        "            y_fake = discriminator(X_fake)\n",
        "            g_loss = adversarial_loss(y_fake)\n",
        "            g_loss.backward()\n",
        "            generator_optimizer.step()\n",
        "\n",
        "            # Train the discriminator\n",
        "            discriminator_optimizer.zero_grad()\n",
        "            y_real = discriminator(X)\n",
        "            y_fake = discriminator(X_fake.detach())\n",
        "            d_loss = discriminator_loss(y_real, y_fake)\n",
        "            d_loss.backward()\n",
        "            discriminator_optimizer.step()\n",
        "\n",
        "generator = Generator(latent_dim=16)\n",
        "discriminator = Discriminator()\n",
        "training_loop(generator, discriminator, train_dataloader, epochs=100, lr=1e-3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXvJvZZfxAtt"
      },
      "source": [
        "### Step 8: Sampling from the GAN\n",
        "\n",
        "In this step, we will sample new data from the trained GAN model. Sampling involves generating new data samples using the generator, which has learned to produce realistic data from random noise vectors.\n",
        "\n",
        "#### Sampling Process\n",
        "1. **Generate Random Noise Vectors**: Sample random noise vectors from a standard normal distribution. These vectors serve as the input to the generator.\n",
        "2. **Generate Data Samples**: Pass the random noise vectors through the generator to produce new data samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLVgf0kjxAtt"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(generator, n_samples):\n",
        "    '''\n",
        "    Sample from the generator.\n",
        "    Args:\n",
        "        generator: nn.Module: The generator model.\n",
        "        n_samples: int: The number of samples to generate.\n",
        "    '''\n",
        "    generator.eval()\n",
        "    noise = None # sample from a standard normal distribution\n",
        "    generated = generator(noise)\n",
        "    return generated\n",
        "\n",
        "# Sample from the generator and plot the results.\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "samples = sample(generator, n_samples=24)\n",
        "plot_dataset(samples, title='Generated samples')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}